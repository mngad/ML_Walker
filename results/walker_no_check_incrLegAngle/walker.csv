Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
5052000,1.0970058,50.44444444444444,20.972134,1.301290942562951,1.301290942562951,27.754295,0.018264076,0.00029696984,0.19898993,0.0009900004,1.0
5064000,1.0966297,160.42857142857142,31.861115,23.53889101743698,23.53889101743698,34.582054,0.019889638,0.000296965,0.19898833,0.0009899845,1.0
5076000,1.0950675,243.91666666666666,49.831978,50.5430818133884,50.5430818133884,47.038784,0.020655056,0.00029695797,0.19898598,0.0009899613,1.0
5088000,1.0940564,358.7391304347826,60.16161,98.85575212538242,98.85575212538242,103.05402,0.01790753,0.00029695124,0.19898374,0.0009899391,1.0
5100000,1.0919998,443.97435897435895,67.70327,216.65666870908305,216.65666870908305,88.77073,0.013768229,0.000296944,0.19898133,0.000989915,1.0
5112000,1.0911776,236.21428571428572,36.02571,76.8007276569094,76.8007276569094,49.289745,0.02000145,0.00029693646,0.1989788,0.0009898903,1.0
5124000,1.0902394,282.5625,50.755302,101.69817519187927,101.69817519187927,52.12296,0.017870197,0.00029692907,0.19897635,0.0009898661,1.0
