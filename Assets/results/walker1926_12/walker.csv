Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
12000,1.4187822,36.95569620253165,0.06196638,0.45382061551179603,0.45382061551179603,0.17246641,0.010817332,0.00029999623,0.19999874,0.0009999875,1.0
24000,1.4183722,45.57805907172996,0.21148989,0.6214285915514001,0.6214285915514001,0.19858691,0.012201534,0.0002999888,0.19999626,0.0009999631,1.0
36000,1.4183245,51.50210970464135,0.28837907,0.7442553367703519,0.7442553367703519,0.22990479,0.010347946,0.00029998174,0.1999939,0.0009999397,1.0
48000,1.4167134,51.45777777777778,0.32765558,0.7929824741001714,0.7929824741001714,0.20621392,0.012493373,0.0002999746,0.19999152,0.0009999161,1.0
60000,1.4151185,64.25,0.5024858,1.2324324408093015,1.2324324408093015,0.24524415,0.0142811155,0.00029996756,0.19998918,0.000999893,1.0
72000,1.4134698,116.05714285714286,0.64534163,1.6523809507489204,1.6523809507489204,0.28553197,0.012860356,0.00029996026,0.19998676,0.0009998688,1.0
